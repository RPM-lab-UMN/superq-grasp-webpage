<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SuperQ-GRASP: Superquadrics-based Grasp Pose Estimation on
        Larger Objects for Mobile-Manipulation">
  <meta name="keywords" content="Manipulation, Large Objects, Superquadrics, Primitive Decomposition">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SuperQ-Grasp</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <script>
    function updateSingleVideo() {
      var object = document.getElementById("single-menu-objects").value;


      var gif1 = document.getElementById("synthetic-data-superq-grasp");
      gif1.src = "media/results/synthetic_data/" + 
                  object +
                  "-superq-grasp.gif"
      var gif2 = document.getElementById("synthetic-data-contact-graspnet");
      gif2.src = "media/results/synthetic_data/" + 
                  object +
                  "-contact-graspnet.gif"
      var gif3 = document.getElementById("synthetic-data-contact-graspnet-depth");
      gif3.src = "media/results/synthetic_data/" + 
                  object +
                  "-contact-graspnet-depth.gif"
    }

    function updateRealWorldVideo() {
      var task = document.getElementById("single-menu-real-world").value;


      var video = document.getElementById("real-world-video");
      video.src = "media/results/real-world/" + 
                  task + 
                  ".mp4"
      video.playbackRate = 1;
      video.play();
    }

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .synthetic-data-image-container {
        display: flex;
        justify-content: space-between;
        align-items: flex-start; /* Align to the top of the container */
    }

    .synthetic-data-image-container figure {
        width: 33%; /* Adjust width for each figure */
        text-align: center; /* Center the caption */
        margin: 10px;
    }

    .synthetic-data-image-container img {
        width: 100%; /* Make image take up the entire figure width */
        height: auto; /* Keep aspect ratio */
    }

    figcaption {
        font-size: 0.9em; /* Smaller font for captions */
        color: #000; /* Black color for captions */
        margin-top: 5px;
    }
  </style>
</head>
<body onload="updateSingleVideo(); updateRealWorldVideo();">

<section class="author">
  <div class="author-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SuperQ-GRASP: <br></h1>
          <h2 class="subtitle is-2 publication-subtitle">Superquadrics-based Grasp Pose Estimation <br>on
            Larger Objects for Mobile-Manipulation</h2>
          <!-- <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.ieee-ras.org/conferences-workshops/technically-co-sponsored/icar">ICRA 2025</a></h3> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank">Xun Tu</a>,</span>
            <span class="author-block">
              <a target="_blank">Karthik Desingh</a>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Minnesota, Twin Cities</span>
          </div>
          <!-- Project under review should not be released yet -->
          <!-- Refer to PerAct if you want to add the links back-->
        </div>
      </div>
    </div>
  </div>
</section>
<br>
<br>
<br>
<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" autoplay muted loop height="100%">
            <source src="media/intro/teaser.mp4"
                    type="video/mp4">
          </video>
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        <span class="dsuperq-grasp">SupereQ-GRASP</span> is a comprehensive pipeline designed specifically to grasp the <b>large</b> objects <br>uncommon in a tabletop scenario based on <b>Primitive Decomposition</b><br></h2>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Abstract</h3>
        <div class="content has-text-justified">
          <p>
            Grasp planning and estimation have been a long-standing research problem in robotics, 
            with two main approaches to find graspable poses on the objects: 1) geometric approach, 
            which relies on 3D models of objects and the gripper to estimate valid grasp poses, and 2) 
            data-driven, learning-based approach, with models trained to identify grasp poses from raw 
            sensor observations. The latter assumes comprehensive geometric coverage during the training phase. 
            However, the data-driven approach is typically biased toward tabletop scenarios and struggle to 
            generalize to out-of-distribution scenarios with larger objects (e.g. chair). Additionally, 
            raw sensor data (e.g. RGB-D data) from a single view of these larger objects is often incomplete 
            and necessitates additional observations. In this paper, we take a geometric approach, leveraging 
            advancements in object modeling (e.g. NeRF) to build an implicit model by taking RGB images from 
            views around the target object. This model enables the extraction of explicit mesh model while also 
            capturing the visual appearance from novel viewpoints that is useful for perception tasks like object 
            detection and pose estimation. We further decompose the NeRF-reconstructed 3D mesh into superquadrics (SQs) 
            - parametric geometric primitives, each mapped to a set of precomputed grasp poses, allowing grasp 
            composition on the target object based on these primitives.Our proposed pipeline overcomes the problems: 
            a) noisy depth and incomplete view of the object, with a modeling step, and b) generalization to objects 
            of any size. 
          </p>

          <p></p>
            We validate the performance of our pipeline on 5 different <b>large objects</b>
            at <b>different poses</b> in real-world experiments using
            <a target="_blank" href="https://bostondynamics.com/products/spot/">SPOT from Boston Dynamics</a>
          </p>
         
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-chair1">
            <video poster="" id="chair1-1" autoplay controls muted loop height="100%">
              <source src="media/intro/chair1-1.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-chair2-1">
            <video poster="" id="chair2-1" autoplay controls muted loop height="100%">
              <source src="media/intro/chair2-1.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-chair2-2">
            <video poster="" id="chair2-2" autoplay controls muted loop height="100%">
              <source src="media/intro/chair2-2.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-luggage-case">
            <video poster="" id="luggage-case" autoplay controls muted loop height="100%">
              <source src="media/intro/luggage-case-1.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-table">
            <video poster="" id="table" autoplay controls muted loop height="70%">
              <source src="media/intro/table-1.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-vacuum-cleaner">
            <video poster="" id="vacuum-cleaner" autoplay controls muted loop height="70%">
              <source src="media/intro/vacuum-cleaner-1.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <!-- More teaser videos -->
      </div>
    </div>
  </div>
</section>


<section class="section">
  <!-- Paper video. -->
  <div class="container is-max-widescreen">
    <div class="column is-two-thirds">
      <h1 class="title is-3">Video</h1>
      <div style="text-align: center; width: 150%">
        <video controls>
          <source style="text-align: center; width: 100%" src="media/intro/video_short.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3">SuperQ-Grasp</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Grasp Pose Estimation</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <p>
          The main contribution of the project is to estimate grasp poses on the large objects, which are uncommon 
          in table scenarios. The key idea is to decompose the target object mesh into several primitive
          shapes, predict grasp poses on the individual primitive shape, and then filter out the invalid
          ones. Here, we use Superquadrics as the primitive shape, and depend on <a target="_blank" 
          href="https://github.com/ChirikjianLab/Marching-Primitives">Marching
          Primitives</a> to decompose the target object mesh into several smaller superquadrics. 
        </p>
        </br>
        </br>
        <figure>
          <p style="text-align:center;"> 
          <img src="media/figures/pipeline.jpg" class="interpolation-image" 
          alt="Overview of the Graps Pose Estimation module"/>
         <figcaption style="text-align:center;">Fig.2 - A comprehensive pipeline specifically
           designed to estimate grasp poses for larger objects.
            By representing an object as a collection of superquadrics,
             the proposed grasp pose estimation method (SuperQ-GRASP) 
             estimates the grasp pose closest to the current gripper by 
             selecting the nearest superquadric and its corresponding valid 
             grasp candidates. Combined with the object detection and pose estimation 
             module, our pipeline enables the mobile manipulator to perform grasping tasks effectively.
         </figcaption>
        </p>
        </figure>
        </br>
        </br>
        <h3 class="title is-4">Real-world Experiments</h3>
          <p class="justify">
            We validate the performance of our pipeline on the robotic platform
            <a target="_blank" href="https://bostondynamics.com/products/spot/">SPOT from Boston Dynamics</a>.
             We use <a target="_blank" href="https://github.com/NVlabs/instant-ngp">instant-NGP</a> to construct the target object mesh.
             Also, unlike synthetic data in simulation, the gripper pose in real-world is unknown in advance.
             To deal with this issue, we depend on GroundingSAM and LoFTR to
             estimate gripper pose. 
          </p>
        <br/>
        <br/>

        <!--/ Re-rendering. -->

        <h2 class="title is-3">Results</h2>

        <h3 class="title is-4">Experiments on synthetic data</h3>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">Qualitative Results on selected Objects
            </h3>
            <font size=1>(Click the videos to open them in a new tab, if you want to see them more clearly) <br>
            <br>
          </font>
            Evaluation on the object
            <div class="select is-small">     
              <select id="single-menu-objects" onchange="updateSingleVideo()">
              <option value="chair1" selected="selected">chair1</option>
              <option value="chair2">chair2</option>
              <option value="chair3">chair3</option>
              <option value="cart1">cart1</option>
              <option value="cart2">cart2</option>
              <option value="cart3">cart3</option>
              <option value="bucket1">bucket1</option>
              <option value="bucket2">bucket2</option>
              <option value="box1">box1</option>
              <option value="box2">box2</option>
              <option value="suitcase1">suitcase1</option>
              <option value="suitcase2">suitcase2</option>
              <option value="table1">table1</option>
              <option value="table2">table2</option>
              <option value="folding-chair">folding-chair</option>
              <option value="chair1-real">chair1-real</option>
              <option value="chair2-real">chair2-real</option>
              <option value="table-real">table-real</option>
              <option value="vacuum-cleaner-real">vacuum-cleaner-real</option>
              <option value="suitcase-real">suitcase-real</option>
            </select>
           
            
            </div>
            
            <br/>
            <br/>

            <div class="synthetic-data-image-container">
              <figure>
                  <img src="./media/results/synthetic_data/chair1-contact-graspnet-depth.gif" alt="Image 1" id="synthetic-data-contact-graspnet-depth" onclick="window.open(this.src, '_blank');">
                  <figcaption>Contact GraspNet+Depth</figcaption>
              </figure>
              <figure>
                <img src="media/results/synthetic_data/chair1-contact-graspnet.gif" alt="Image 2" id="synthetic-data-contact-graspnet" onclick="window.open(this.src, '_blank');">
                  <figcaption>Contact GraspNet+Mesh</figcaption>
              </figure>
              <figure>
                                
                <img src="media/results/synthetic_data/chair1-superq-grasp.gif" alt="Image 3" id="synthetic-data-superq-grasp" onclick="window.open(this.src, '_blank');">
                  <figcaption>Superq Grasp</figcaption>
              </figure>
            </div>
            
          </div>
        </div>
        <b>NOTE:</b><br>
        <font color="red">Red: </font> invalid grasp poses;  <font  color="#87ed2d">Green: </font>valid grasp poses <br>
        <font color="blue">Blue dots: </font> observed depth point cloud as a partial view of the object
        <br>
        <br>
      <br>
      <br>
        <h3 class="title is-4">Real-world Experiments</h3>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">Real-world Experiment examples</h3>

            Visualize the real-world experiment for   
            <div class="select is-small is-rounded">     
              <select id="single-menu-real-world" onchange="updateRealWorldVideo()">
              <option value="chair1" selected="selected">chair1</option>
              <option value="chair2">chair2</option>
              <option value="vacuum-cleaner">vacuum-cleaner</option>
              <option value="luggage-case">luggage-case</option>
              <option value="table">table</option>
              </select>
            </div>

            <video  id="real-world-video" controls>
              <source style="text-align: center; width: 100%" src="media/results/real-world/chair1.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video>

      </div>
    </div>
  </div>
</section>



<br>
<br>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{shridhar2022peract,
  title     = {Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation}, 
  author    = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  booktitle = {Proceedings of the 6th Conference on Robot Learning (CoRL)},
  year      = {2022},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://peract.github.io/">PerAct</a> made primarily by <a href="https://mohitshridhar.com/">Mohit Shridhar</a> and <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by <a href="https://keunhong.com/">Keunhong Park</a>. Great Appreciations to their excellent works!
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
