<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SuperQ-GRASP: Superquadrics-based Grasp Pose Estimation on
        Larger Objects for Mobile-Manipulation">
  <meta name="keywords" content="Manipulation, Large Objects, Superquadrics, Primitive Decomposition">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SuperQ-Grasp</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <script>
    function updateSingleVideo() {
      var method = document.getElementById("single-menu-methods").value;
      var object = document.getElementById("single-menu-objects").value;


      var gif = document.getElementById("synthetic-data-result-gif");
      gif.src = "media/results/synthetic_data/" + 
                  object +
                  "-" +
                  method + 
                  ".gif"
    }

    function updateQpredVideo() {
      var task = document.getElementById("single-menu-qpred").value;

      console.log("qpred", task)

      var video = document.getElementById("q-pred-video");
      video.src = "media/results/qpred/" + 
                  task + 
                  ".mp4"
      video.playbackRate = 1.75;
      video.play();
    }

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateSingleVideo(); updateQpredVideo();">

<section class="author">
  <div class="author-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SuperQ-GRASP: <br></h1>
          <h2 class="subtitle is-2 publication-subtitle">Superquadrics-based Grasp Pose Estimation <br>on
            Larger Objects for Mobile-Manipulation</h2>
          <!-- <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.ieee-ras.org/conferences-workshops/technically-co-sponsored/icar">ICRA 2025</a></h3> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank">Xun Tu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://karthikdesingh.com/">Karthik Desingh</a><sup>2</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Minnesota, Twin Cities</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="paper/Tu_elal_ICRA_2025.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Arxiv Link. -->
            <!-- <span class="link-block">
              <a target="_blank" href="https://arxiv.org/abs/2209.05451"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file"></i>
                </span>
                <span>ArXiv</span>
              </a>
            </span> -->

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href="https://www.youtube.com"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>


            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com/TuXun1999/multi-purpose-representation"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>

            </div>
          </div>
<!--           <br>
          <br> -->


        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <img src="media/figures/qual_comp.jpg" class="interpolation-image" 
         alt="Place holder for qualitative results." />
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        <span class="dsuperq-grasp">SupereQ-Grasp</span> is a comprehensive pipeline designed specifically to grasp the <b>large</b> objects uncommon in a tabletop scenario based on <b>Primitive Decomposition</b></h2>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-chair1">
          <video poster="" id="chair1-1" autoplay controls muted loop height="100%">
            <source src="media/intro/chair1-1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair2-1">
          <video poster="" id="chair2-1" autoplay controls muted loop height="100%">
            <source src="media/intro/chair2-1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair2-2">
          <video poster="" id="chair2-2" autoplay controls muted loop height="100%">
            <source src="media/intro/chair2-2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-luggage-case">
          <video poster="" id="luggage-case" autoplay controls muted loop height="100%">
            <source src="media/intro/luggage-case-1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-table">
          <video poster="" id="table" autoplay controls muted loop height="100%">
            <source src="media/intro/table-1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-vacuum-cleaner">
          <video poster="" id="vacuum-cleaner" autoplay controls muted loop height="100%">
            <source src="media/intro/vacuum-cleaner-1.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<h2 class="subtitle has-text-centered">
</br>
  We also validate the performance of our pipeline on 5 different <b>large objects</b> <br>
  at <b>different poses</b> in real-world experiments using
  <a target="_blank" href="https://bostondynamics.com/products/spot/">SPOT from Boston Dynamics</a>
</h2>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Grasp planning and estimation has been a long-
            standing research problem in robotics, with two main approaches
            to find graspable poses on the objects:
            <ul>
              <li>Geometric approach, which rely on 3D models of objects 
                and gripper to estimate valid grasp poses</li>
              <li>Data-driven, learning-based approach,
                with models trained to identify grasp poses on raw sensor ob-
                servations</li>
            </ul>
            The later assumes comprehensive geometric coverage
            during training phase, which, however, are typically biased toward table-
            top scenarios and struggle to generalize to out-of-distribution
            scenarios with larger objects (e.g. chair). Additionally, raw
            sensor data (e.g. RGB-D data) from a single view of these
            larger objects is often incomplete and noisy and necessitates
            additional observations. 
          </p>
          <p>
            In this paper, we take a geometric approach, leveraging
            advancements in object modeling (e.g. 
            <a target="_blank" href="https://www.matthewtancik.com/nerf">NeRF</a>) 
            to build an implicit
            model by taking RGB images from views around the target
            object. This model enables the extraction of explicit mesh model
            while also capturing the visual appearance from novel viewpoints
            that is useful for perception tasks like object detection and pose
            estimation. We further decompose the NeRF-reconstructed 3D
            mesh into <a target="_blank" href="https://ieeexplore.ieee.org/document/1673799">
              Superquadrics
            </a>(SQs) - parametric geometric primitives,
            each mapped to a set of precomputed grasp poses, allowing
            grasp composition on the target object based on these primitives.
            Our proposed pipeline overcomes the problems: 
            <ul>
              <li>Noisy depth and incomplete view of the object, with a modelling step</li>
              <li>Generalization to objects of any size, especially the ones larger in size and uncommon in tabletop scenarios</li>
            </ul>
            
          </p>
         
        </div>
      </div>
    </div>
    <br>
    <br>
    <!--/ Abstract. -->

  </div>

  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <iframe src="https://www.youtube.com"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="dperact">SuperQ-Grasp</span></h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Superquadrics</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <p>
         Superquadrics is a series of primitive shape described by the implicit equation:
          \[\left(\left(\frac{x}{a_x}\right)^{\frac{2}{\varepsilon_2}} + \left(\frac{y}{a_y}\right)^{\varepsilon_2}\right)^{\frac{\varepsilon_2}{\varepsilon_1}} + \left(\frac{z}{a_z}\right)^{\frac{2}{\varepsilon_1}}=1\]
         where \(\varepsilon_1, \varepsilon_2, a_1, a_2, a_3\)are parameters. Only five parameters as each superquadric is specifed by, 
         it can cover a range of different shapes, including cylinderic or cubic ones
        </p>
        </br>
        </br>
        <figure>
          <p style="text-align:center;"> 
          <img src="media/figures/superquadrics.png" class="interpolation-image" 
          alt="Superquadrics with different parameters"/>
         <figcaption style="text-align:center;">Fig.1 - Superquadrics with different parameters 
          (<a target="_blank" href="https://cse.buffalo.edu/~jryde/cse673/files/superquadrics.pdf">source</a>)
         </figcaption>
        </p>
        </figure>
       
        </br>
        
        </br>
        <h3 class="title is-4">Grasp Pose Estimation</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <p>
          The main contribution of the project is to estimate grasp poses on the large objects, which are uncommon 
          in table scenarios. The key idea is to decompose the target object mesh into several primitive
          shapes, predict grasp poses on the individual primitive shape, and then filter out the invalid
          ones. Here, we use Superquadrics as the primitive shape, and depend on <a target="_blank" 
          href="https://github.com/ChirikjianLab/Marching-Primitives">Marching
          Primitives</a> to decompose the target object mesh into several smaller superquadrics. 
        </p>
        </br>
        </br>
        <figure>
          <p style="text-align:center;"> 
          <img src="media/figures/system.jpg" class="interpolation-image" 
          alt="Overview of the Graps Pose Estimation module"/>
         <figcaption style="text-align:center;">Fig.2 - Overview of the Graps Pose Estimation method
         </figcaption>
        </p>
        </figure>
        </br>
        </br>
          <p>
              
          </p>
        </br>
        </br>
        <h3 class="title is-4">Real-world Experiments</h3>
          <p class="justify">
            We validate the performance of our pipeline on the robotic platform
            <a target="_blank" href="https://bostondynamics.com/products/spot/">SPOT from Boston Dynamics</a>.
             We use <a target="_blank" href="https://github.com/NVlabs/instant-ngp">instant-NGP</a> to construct the target object mesh.
             Also, unlike synthetic data in simulation, the gripper pose in real-world is unknown in advance.
             To deal with this issue, we depend on GroundingSAM and LoFTR to
             estimate gripper pose. 
             <br>
             <br>
             <figure>
              <p style="text-align:center;"> 
              <img src="media/figures/pipeline.jpg" class="interpolation-image" 
              alt="Overview of the whole pipeline"/>
             <figcaption style="text-align:center;">Fig.3 - Overview of the whole pipeline
             </figcaption>
            </p>
            </figure>
          </p>
        <br/>
        <br/>

        <!--/ Re-rendering. -->

        <h2 class="title is-3">Results</h2>

        <h3 class="title is-4">Experiments on synthetic data</h3>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">Qualitative Results on selected Objects</h3>

            Method
            <div class="select is-small">
              <select id="single-menu-methods" onchange="updateSingleVideo()">
              <option value="superq-grasp" selected="selected">superq-grasp</option>
              <option value="contact-graspnet">contact-graspnet</option>
              <option value="contact-graspnet-depth">contact-graspnet-depth</option>
              </select>
            </div>
            evaluated on the object
            <div class="select is-small">     
              <select id="single-menu-objects" onchange="updateSingleVideo()">
              <option value="chair1" selected="selected">chair1</option>
              <option value="chair2">chair2</option>
              <option value="chair3">chair3</option>
              <option value="cart1">cart1</option>
              <option value="cart2">cart2</option>
              <option value="cart3">cart3</option>
              <option value="bucket1">bucket1</option>
              <option value="bucket2">bucket2</option>
              <option value="box1">box1</option>
              <option value="box2">box2</option>
              <option value="suitcase1">suitcase1</option>
              <option value="suitcase2">suitcase2</option>
              <option value="table1">table1</option>
              <option value="table2">table2</option>
              <option value="folding-chair1">folding-chair</option>
              <option value="chair1-real">chair1-real</option>
              <option value="chair2-real">chair2-real</option>
              <option value="table-real">table-real</option>
              <option value="vacuum-cleaner-real">vacuum-cleaner-real</option>
              <option value="suitcase-real">suitcase-real</option>
            </select>
            </div>
            
            <br/>
            <br/>

            <img id="synthetic-data-result-gif" src="media/results/synthetic_data/chair1-superq-grasp.gif" class="interpolation-image" 
                  alt="Result on the selected method and object" />
          </div>
        </div>
        </br>
        </br>

        <h3 class="title is-4">Real-world Experiments</h3>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">Examples</h3>

            Visualize performance for
            

      </div>
    </div>
  </div>
</section>

<video id="q-pred-video"
       muted
       autoplay
       loop
       width="100%">
  <source src="media/results/qpred/tomat.mp4"
          type="video/mp4">
</video>

<br>
<br>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{shridhar2022peract,
  title     = {Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation}, 
  author    = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  booktitle = {Proceedings of the 6th Conference on Robot Learning (CoRL)},
  year      = {2022},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://peract.github.io/">PerAct</a> made primarily by <a href="https://mohitshridhar.com/">Mohit Shridhar</a> and <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by <a href="https://keunhong.com/">Keunhong Park</a>. Great Appreciations to their excellent works!
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
