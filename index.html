<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SuperQ-GRASP: Superquadrics-based Grasp Pose Estimation on
        Larger Objects for Mobile-Manipulation">
  <meta name="keywords" content="Manipulation, Large Objects, Superquadrics, Primitive Decomposition">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SuperQ-Grasp</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <script>
    function updateSingleVideo() {
      var demo = document.getElementById("single-menu-demos").value;
      var task = document.getElementById("single-menu-tasks").value;
      var inst = document.getElementById("single-menu-instances").value;

      console.log("single", demo, task, inst)

      var video = document.getElementById("multi-task-result-video");
      video.src = "media/results/sim_rollouts/" + 
                  "n" +
                  demo +
                  "-" +
                  task +
                  "-" +
                  inst +
                  ".mp4"
      video.playbackRate = 1.75;
      video.play();
    }

    function updateQpredVideo() {
      var task = document.getElementById("single-menu-qpred").value;

      console.log("qpred", task)

      var video = document.getElementById("q-pred-video");
      video.src = "media/results/qpred/" + 
                  task + 
                  ".mp4"
      video.playbackRate = 1.75;
      video.play();
    }

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateSingleVideo(); updateQpredVideo();">

<section class="author">
  <div class="author-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SuperQ-GRASP: <br></h1>
          <h2 class="subtitle is-2 publication-subtitle">Superquadrics-based Grasp Pose Estimation <br>on
            Larger Objects for Mobile-Manipulation</h2>
          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.ieee-ras.org/conferences-workshops/technically-co-sponsored/icar">ICRA 2025</a></h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank">Xun Tu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://karthikdesingh.com/">Karthik Desingh</a><sup>2</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Minnesota, Twin Cities</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="paper/Tu_elal_ICRA_2025.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Arxiv Link. -->
            <span class="link-block">
              <a target="_blank" href="https://arxiv.org/abs/2209.05451"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file"></i>
                </span>
                <span>ArXiv</span>
              </a>
            </span>

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href="https://www.youtube.com/watch?v=TB0g52N-3_Y"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>


            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com/TuXun1999/multi-purpose-representation"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>

            </div>
          </div>
<!--           <br>
          <br> -->


        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <img src="media/figures/qual_comp.jpg" class="interpolation-image" 
         alt="Place holder for qualitative results." />
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        <span class="dsuperq-grasp">SupereQ-Grasp</span> is a comprehensive pipeline designed specifically to grasp the <b>large</b> objects uncommon in a tabletop scenario based on <b>Primitive Decomposition</b></h2>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-chair1">
          <video poster="" id="chair1-1" autoplay muted loop height="100%">
            <source src="media/intro/chair1-1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair2-1">
          <video poster="" id="chair2-1" autoplay muted loop height="100%">
            <source src="media/intro/chair2-1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair2-2">
          <video poster="" id="chair2-2" autoplay muted loop height="100%">
            <source src="media/intro/chair2-2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-luggage-case">
          <video poster="" id="luggage-case" autoplay muted loop height="100%">
            <source src="media/intro/luggage-case-1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-table">
          <video poster="" id="table" autoplay muted loop height="100%">
            <source src="media/intro/table-1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-vacuum-cleaner">
          <video poster="" id="shiba" autoplay muted loop height="100%">
            <source src="media/intro/vacuum-cleaner-1.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<h2 class="subtitle has-text-centered">
</br>
  We also validate the performance of our pipeline on 5 different <b>large objects</b> <br>
  at <b>different poses</b> in real-world experiments using
  <a target="_blank" href="https://bostondynamics.com/products/spot/">SPOT from Boston Dynamics</a>
</h2>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Grasp planning and estimation has been a long-
            standing research problem in robotics, with two main approaches
            to find graspable poses on the objects:
            <ul>
              <li>Geometric approach, which rely on 3D models of objects 
                and gripper to estimate valid grasp poses</li>
              <li>Data-driven, learning-based approach,
                with models trained to identify grasp poses on raw sensor ob-
                servations</li>
            </ul>
            The later assumes comprehensive geometric coverage
            during training phase, which, however, are typically biased toward table-
            top scenarios and struggle to generalize to out-of-distribution
            scenarios with larger objects (e.g. chair). Additionally, raw
            sensor data (e.g. RGB-D data) from a single view of these
            larger objects is often incomplete and noisy and necessitates
            additional observations. 
          </p>
          <p>
            In this paper, we take a geometric approach, leveraging
            advancements in object modeling (e.g. 
            <a target="_blank" href="https://www.matthewtancik.com/nerf">NeRF</a>) 
            to build an implicit
            model by taking RGB images from views around the target
            object. This model enables the extraction of explicit mesh model
            while also capturing the visual appearance from novel viewpoints
            that is useful for perception tasks like object detection and pose
            estimation. We further decompose the NeRF-reconstructed 3D
            mesh into <a target="_blank" href="https://ieeexplore.ieee.org/document/1673799">
              Superquadrics
            </a>(SQs) - parametric geometric primitives,
            each mapped to a set of precomputed grasp poses, allowing
            grasp composition on the target object based on these primitives.
            Our proposed pipeline overcomes the problems: 
            <ul>
              <li>Noisy depth and incomplete view of the object, with a modelling step</li>
              <li>Generalization to objects of any size, especially the ones larger in size and uncommon in tabletop scenarios</li>
            </ul>
            
          </p>
         
        </div>
      </div>
    </div>
    <br>
    <br>
    <!--/ Abstract. -->

  </div>

  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <iframe src="https://www.youtube.com/embed/TB0g52N-3_Y?rel=0&amp;showinfo=0"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="dperact">SuperQ-Grasp</span></h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Superquadrics</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <p>
         Superquadrics is a series of primitive shape described by the implicit equation:
          \[\left(\left(\frac{x}{a_x}\right)^{\frac{2}{\varepsilon_2}} + \left(\frac{y}{a_y}\right)^{\varepsilon_2}\right)^{\frac{\varepsilon_2}{\varepsilon_1}} + \left(\frac{z}{a_z}\right)^{\frac{2}{\varepsilon_1}}=1\]
         where \(\varepsilon_1, \varepsilon_2, a_1, a_2, a_3\)are parameters. Only five parameters as each superquadric is specifed by, 
         it can cover a range of different shapes, including cylinderic or cubic ones
        </p>
        </br>
        </br>
        <figure>
          <p style="text-align:center;"> 
          <img src="media/figures/superquadrics.png" class="interpolation-image" 
          alt="Superquadrics with different parameters" align="center"/>
         <figcaption style="text-align:center;">Fig.1 - Superquadrics with different parameters 
          (<a target="_blank" href="https://cse.buffalo.edu/~jryde/cse673/files/superquadrics.pdf">source</a>)
         </figcaption>
        </p>
        </figure>
       
        </br>
        
        </br>
        <h3 class="title is-4">Grasp Pose Estimation</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <p>
          The main contribution of the project is to 
        </p>
        </br>
        </br>
        <img src="media/figures/system.jpg" class="interpolation-image" 
         alt="Interpolate start reference image." />
        </br>
        </br>
          <p>
              
          </p>
        </br>
        </br>
        <h3 class="title is-4">Real-world Experiments</h3>
          <p class="justify">
            We validate the performance of our pipeline on the robotic platform
            <a target="_blank" href="https://bostondynamics.com/products/spot/">SPOT from Boston Dynamics</a>
             
          </p>
        <br/>
        <br/>

        <!--/ Re-rendering. -->

        <h2 class="title is-3">Results</h2>

        <h3 class="title is-4">Simulation Results</h3>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">One Multi-Task Transformer</h3>

            Trained with
            <div class="select is-small">
              <select id="single-menu-demos" onchange="updateSingleVideo()">
              <option value="10">10</option>
              <option value="100" selected="selected">100</option>
              </select>
            </div>
            demos per task, evaluated on 
            <div class="select is-small">     
              <select id="single-menu-tasks" onchange="updateSingleVideo()">
              <option value="open_drawer" selected="selected">open drawer</option>
              <option value="slide_block">slide block</option>
              <option value="sweep_to_dustpan">sweep to dustpan</option>
              <option value="meat_off_grill">meat off grill</option>
              <option value="turn_tap">turn tap</option>
              <option value="put_in_drawer">put in drawer</option>
              <option value="close_jar">close jar</option>
              <option value="drag_stick">drag stick</option>
              <option value="stack_blocks">stack blocks</option>
              <option value="screw_bulb">screw bulb</option>
              <option value="put_in_safe">put in safe</option>
              <option value="place_wine">place wine</option>
              <option value="put_in_cupboard">put in cupboard</option>
              <option value="sort_shape">sort shape</option>
              <option value="push_buttons">push buttons</option>
              <option value="insert_peg">insert peg</option>
              <option value="stack_cups">stack cups</option>
              <option value="place_cups">place cups</option>
              </select>
            </div>
            episode
            <div class="select is-small">
              <select id="single-menu-instances" onchange="updateSingleVideo()">
              <option value="s1">01</option>
              <option value="s2" selected="selected">02</option>
              <option value="s3">03</option>
              <option value="s4">04</option>
              <option value="s5">05</option>
              </select>
            </div>
            <br/>
            <br/>

            <video id="multi-task-result-video"
                   muted
                   autoplay
                   loop
                   width="100%">
              <source src="media/results/sim_rollouts/n10-open_drawer-s2.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        </br>
        </br>

        <h3 class="title is-4">Action Predictions</h3>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">Q-Prediction Examples</h3>

            Visualize predictions for   
            <div class="select is-small is-rounded">     
              <select id="single-menu-qpred" onchange="updateQpredVideo()">
              <option value="tomato" selected="selected">"put the tomatoes in the top bin"</option>
              <option value="stick">"hit the green ball with the stick"</option>
              <option value="handsan">"press the hand san"</option>
              <option value="tape">"put the tape in the top drawer"</option>
              </select>
            </div>
          </div>

      </div>
    </div>
  </div>
</section>

<video id="q-pred-video"
       muted
       autoplay
       loop
       width="100%">
  <source src="media/results/qpred/tomato.mp4"
          type="video/mp4">
</video>

<br>
<br>


<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{shridhar2022peract,
  title     = {Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation}, 
  author    = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  booktitle = {Proceedings of the 6th Conference on Robot Learning (CoRL)},
  year      = {2022},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://peract.github.io/">PerAct</a> made primarily by <a href="https://mohitshridhar.com/">Mohit Shridhar</a> and <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by <a href="https://keunhong.com/">Keunhong Park</a>. Great Appreciations to their excellent works!
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
